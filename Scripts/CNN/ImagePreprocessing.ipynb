{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e8f8e36-18b5-44bf-ab7b-ec6bdb97594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "# we are using efficient net B3 but we can downgrade if it is too computationally expensive\n",
    "from tensorflow. keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1294d159-13a9-4f2d-b909-2bbe3f2484f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' in this notebook we can preprocess images for the CNN. all the images are super nice and prestine, biut we want them to be distorted \\nto an extent or \"preprocessed\" so the CNN can recognize images pf even low quality'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' in this notebook we can preprocess images for the CNN. all the images are super nice and prestine, biut we want them to be distorted \n",
    "to an extent or \"preprocessed\" so the CNN can recognize images pf even low quality'''\n",
    "\n",
    "# the model takes images of size 224,224,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1958e6-8994-41e5-a2f6-66d725810c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths \n",
    "Test_images = '/Users/melissaaprilcastro/FeatherFind/Data/CUB_200_2011/Test'\n",
    "Train_images = '/Users/melissaaprilcastro/FeatherFind/Data/CUB_200_2011/Train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64496368-b110-46e9-9e79-f74893064525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: Scott_Oriole_0018_795840.jpg\n",
      "  - Dimensions: 142 x 160\n",
      "  - Channels: 3\n",
      "  - File size: 9885 bytes\n",
      "\n",
      "Image: Cedar_Waxwing_0075_179114.jpg\n",
      "  - Dimensions: 388 x 500\n",
      "  - Channels: 3\n",
      "  - File size: 183677 bytes\n",
      "\n",
      "Image: Brewer_Sparrow_0052_107478.jpg\n",
      "  - Dimensions: 333 x 500\n",
      "  - Channels: 3\n",
      "  - File size: 127367 bytes\n",
      "\n",
      "Image: Black_Throated_Blue_Warbler_0061_161667.jpg\n",
      "  - Dimensions: 500 x 349\n",
      "  - Channels: 3\n",
      "  - File size: 122929 bytes\n",
      "\n",
      "Image: Kentucky_Warbler_0066_165290.jpg\n",
      "  - Dimensions: 500 x 350\n",
      "  - Channels: 3\n",
      "  - File size: 102009 bytes\n",
      "\n",
      "Image: Northern_Fulmar_0052_43857.jpg\n",
      "  - Dimensions: 500 x 333\n",
      "  - Channels: 3\n",
      "  - File size: 46576 bytes\n",
      "\n",
      "Image: Sage_Thrasher_0093_155501.jpg\n",
      "  - Dimensions: 500 x 333\n",
      "  - Channels: 3\n",
      "  - File size: 136660 bytes\n",
      "\n",
      "Image: Purple_Finch_0006_27950.jpg\n",
      "  - Dimensions: 500 x 500\n",
      "  - Channels: 3\n",
      "  - File size: 133371 bytes\n",
      "\n",
      "Image: Yellow_Bellied_Flycatcher_0062_42693.jpg\n",
      "  - Dimensions: 403 x 500\n",
      "  - Channels: 3\n",
      "  - File size: 141646 bytes\n",
      "\n",
      "Image: Black_Throated_Blue_Warbler_0101_161510.jpg\n",
      "  - Dimensions: 500 x 333\n",
      "  - Channels: 3\n",
      "  - File size: 119510 bytes\n",
      "\n",
      "Image: Horned_Puffin_0004_100733.jpg\n",
      "  - Dimensions: 500 x 450\n",
      "  - Channels: 3\n",
      "  - File size: 144149 bytes\n",
      "\n",
      "Image: Black_Billed_Cuckoo_0023_26258.jpg\n",
      "  - Dimensions: 500 x 375\n",
      "  - Channels: 3\n",
      "  - File size: 170674 bytes\n",
      "\n",
      "Image: Scissor_Tailed_Flycatcher_0059_41998.jpg\n",
      "  - Dimensions: 500 x 400\n",
      "  - Channels: 3\n",
      "  - File size: 50319 bytes\n",
      "\n",
      "Image: Myrtle_Warbler_0020_166997.jpg\n",
      "  - Dimensions: 500 x 403\n",
      "  - Channels: 3\n",
      "  - File size: 134997 bytes\n",
      "\n",
      "Image: Least_Auklet_0050_1924.jpg\n",
      "  - Dimensions: 400 x 500\n",
      "  - Channels: 3\n",
      "  - File size: 127093 bytes\n",
      "\n",
      "Image: Brewer_Sparrow_0076_107393.jpg\n",
      "  - Dimensions: 333 x 500\n",
      "  - Channels: 3\n",
      "  - File size: 80195 bytes\n",
      "\n",
      "Image: Western_Grebe_0046_36118.jpg\n",
      "  - Dimensions: 500 x 469\n",
      "  - Channels: 3\n",
      "  - File size: 150081 bytes\n",
      "\n",
      "Image: Black_Footed_Albatross_0088_796133.jpg\n",
      "  - Dimensions: 335 x 400\n",
      "  - Channels: 3\n",
      "  - File size: 9605 bytes\n",
      "\n",
      "Image: Hooded_Merganser_0012_796748.jpg\n",
      "  - Dimensions: 484 x 460\n",
      "  - Channels: 3\n",
      "  - File size: 87594 bytes\n",
      "\n",
      "Image: Blue_Headed_Vireo_0039_156397.jpg\n",
      "  - Dimensions: 500 x 500\n",
      "  - Channels: 3\n",
      "  - File size: 173592 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function to collect all image paths from nested subdirectories\n",
    "def get_image_paths(folder):\n",
    "    image_paths = []\n",
    "    # Traverse the directory and subdirectories\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for image in files:\n",
    "            # Check if the file is an image by its extension\n",
    "            if image.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff')):\n",
    "                image_paths.append(os.path.join(root, image))\n",
    "    return image_paths  # Return the list of image paths\n",
    "\n",
    "# function to check image dimensions and size of a certain amt of images\n",
    "def check_random_image_sizes(folder, num_samples=10):\n",
    "    # Get all image files from nested subdirectories\n",
    "    image_paths = get_image_paths(folder)\n",
    "    \n",
    "    # check if there are enough images to sample (there are lol)\n",
    "    if len(image_paths) < num_samples:\n",
    "        print(f\"Only found {len(image_paths)} images, processing all of them.\")\n",
    "        selected_images = image_paths\n",
    "    else:\n",
    "        # randomly select a subset of images\n",
    "        selected_images = random.sample(image_paths, num_samples)\n",
    "    \n",
    "    # process each selected image\n",
    "    for file_path in selected_images:\n",
    "        try:\n",
    "            # read the image using OpenCV\n",
    "            img = cv2.imread(file_path)\n",
    "            \n",
    "            # check if image was loaded correctly\n",
    "            if img is not None:\n",
    "                # get dimensions: height, width, and number of channels\n",
    "                height, width, channels = img.shape\n",
    "                \n",
    "                # get file size in bytes\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                \n",
    "                print(f\"Image: {os.path.basename(file_path)}\")\n",
    "                print(f\"  - Dimensions: {width} x {height}\")\n",
    "                print(f\"  - Channels: {channels}\")\n",
    "                print(f\"  - File size: {file_size} bytes\\n\")\n",
    "            else:\n",
    "                print(f\"Could not read image: {os.path.basename(file_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {os.path.basename(file_path)}: {e}\")\n",
    "\n",
    "# run the function to check 10 random images from Train and Test directories\n",
    "check_random_image_sizes(Train_images, num_samples=10)\n",
    "check_random_image_sizes(Test_images, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da91b7ba-6ef6-4aef-a41c-8a92d4d63ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eef65c5a-af8b-4fc1-91b6-1aeb976e9526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base this off the model u use\n",
    "IMG_SIZE = 300\n",
    "\n",
    "SIZE = (IMG_SIZE, IMG_SIZE)\n",
    "# base this off the nu,ber of classification species\n",
    "NUM_CLASSES = 200\n",
    "\n",
    "# now we make a validation set too, using image generator\n",
    "data_gen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    # we want to make a validation set too, make it abt 20% of training data\n",
    "    validation_split = .2,\n",
    "    # this parameter, to my understanding, is to help the model better view distorted images / make the model more robust\n",
    "    shear_range = .2,\n",
    "    zoom_range = .2,\n",
    "    horizontal_flip = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9134a063-9756-44cb-ac41-c02b47328553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4869 images belonging to 200 classes.\n",
      "Found 1125 images belonging to 200 classes.\n",
      "Found 5794 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "# lets apply the data generator to the train set\n",
    "train_data = data_gen.flow_from_directory(\n",
    "    # remember we have the file paths defined above so we use them here\n",
    "    Train_images,\n",
    "    target_size = SIZE,\n",
    "    color_mode='rgb',\n",
    "    # from what i understand, this will be the amount of training cycle iterations\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'training',\n",
    "    # pictures shown in random order\n",
    "    shuffle = True,\n",
    "    # seed is 42 to keep consistency\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "val_data = data_gen.flow_from_directory(\n",
    "    # remember we have the file paths defined above so we use them here\n",
    "    Train_images,\n",
    "    target_size = SIZE,\n",
    "    color_mode='rgb',\n",
    "    # from what i understand, this will be the amount of training cycle iterations\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical',\n",
    "    # only real change between this and train data above\n",
    "    subset = 'validation',\n",
    "    # pictures shown in random order\n",
    "    shuffle = True,\n",
    "    # seed is 42 to keep consistency\n",
    "    seed = 42\n",
    ")\n",
    "# for test set, we keep it simple, only resize it\n",
    "test_data = ImageDataGenerator(\n",
    "    rescale = 1./255\n",
    ")\n",
    "\n",
    "# now let's load in the test data with the datagen specifictions\n",
    "test_data = test_data.flow_from_directory(\n",
    "    Test_images,\n",
    "    target_size = SIZE,\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical',\n",
    "    color_mode = 'rgb',\n",
    "    # keep it in order for test images\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7349227c-9b37-412a-9af1-922561942bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">10,783,535</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">196,736</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,800</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb3 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m1536\u001b[0m)   │    \u001b[38;5;34m10,783,535\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m196,736\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │        \u001b[38;5;34m25,800\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,006,071</span> (41.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,006,071\u001b[0m (41.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,918,768</span> (41.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,918,768\u001b[0m (41.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">87,303</span> (341.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m87,303\u001b[0m (341.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# now lets built the basic model\n",
    "base_model = EfficientNetB3(\n",
    "    input_shape = (IMG_SIZE, IMG_SIZE, 3),\n",
    "    # we choose False when we want to fine tune more intensly\n",
    "    include_top = False,\n",
    "    weights = 'imagenet'\n",
    ")\n",
    "\n",
    "#we're adding custom layers on top of model\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation = 'relu'),\n",
    "    # this should help with overfitting\n",
    "    layers.Dropout(.5),\n",
    "    layers.Dense(NUM_CLASSES, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# now lets compile\n",
    "model.compile(\n",
    "    optimizer = optimizers.Adam(0.0001),\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b09f512-c34f-4f33-9d68-11c14d6a0065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m 42/153\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13:24\u001b[0m 7s/step - accuracy: 0.0063 - loss: 5.3558"
     ]
    }
   ],
   "source": [
    "# now we train omg\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    # i dont think we neeed this: steps_per_epoch = len(train_images),\n",
    "    # will iterate over training data ten times\n",
    "    epochs = 10,\n",
    "    #model uses it to test performance each time\n",
    "    validation_data = val_data,\n",
    "    callbacks = [\n",
    "        # will stop model if it stops improving, prevents overfitting\n",
    "        EarlyStopping(monitor = 'val_loss',\n",
    "                      # waits 5 epochs before stopping\n",
    "                      patience = 5,\n",
    "                      # after stopping it gets best parameters\n",
    "                      restore_best_weights = True),\n",
    "        # redices learning rate when validation stops\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, mode='min')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3d543-985b-411f-a75e-943f7f12a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Adjust the Learning Rate:\n",
    "set initial learning rate lower, e.g., 0.0001, and let ReduceLROnPlateau adjust as necessary.\n",
    "\n",
    "Class Weights:\n",
    "Calculate class weights and pass them to model.fit() to handle class imbalance.\n",
    "\n",
    "Gradual Fine-Tuning:\n",
    "Freeze the base model initially and train only the added dense layers. Gradually unfreeze layers of the base model in steps.\n",
    "\n",
    "Increase Training Data:\n",
    "Use extensive data augmentation, or if possible, collect more data to bolster model training.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b35e54-e610-4b3d-bc13-1742b66f2e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNN_work",
   "language": "python",
   "name": "cnn_work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
