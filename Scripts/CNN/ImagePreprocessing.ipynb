{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e8f8e36-18b5-44bf-ab7b-ec6bdb97594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "# we are using efficient net B3 but we can downgrade if it is too computationally expensive\n",
    "from tensorflow. keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1294d159-13a9-4f2d-b909-2bbe3f2484f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' in this notebook we can preprocess images for the CNN. all the images are super nice and prestine, biut we want them to be distorted \\nto an extent or \"preprocessed\" so the CNN can recognize images pf even low quality'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' in this notebook we can preprocess images for the CNN. all the images are super nice and prestine, biut we want them to be distorted \n",
    "to an extent or \"preprocessed\" so the CNN can recognize images pf even low quality'''\n",
    "\n",
    "# the model takes images of size 224,224,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e1958e6-8994-41e5-a2f6-66d725810c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths \n",
    "Test_images = '/Users/melissaaprilcastro/FeatherFind/Data/CUB_200_2011/Test'\n",
    "Train_images = '/Users/melissaaprilcastro/FeatherFind/Data/CUB_200_2011/Train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64496368-b110-46e9-9e79-f74893064525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: Henslow_Sparrow_0058_796616.jpg\n",
      "  - Dimensions: 300 x 380\n",
      "  - Channels: 3\n",
      "  - File size: 17798 bytes\n",
      "\n",
      "Image: Mockingbird_0078_80426.jpg\n",
      "  - Dimensions: 500 x 375\n",
      "  - Channels: 3\n",
      "  - File size: 51724 bytes\n",
      "\n",
      "Image: Common_Yellowthroat_0035_190567.jpg\n",
      "  - Dimensions: 500 x 407\n",
      "  - Channels: 3\n",
      "  - File size: 95302 bytes\n",
      "\n",
      "Image: Western_Meadowlark_0120_77834.jpg\n",
      "  - Dimensions: 500 x 333\n",
      "  - Channels: 3\n",
      "  - File size: 71278 bytes\n",
      "\n",
      "Image: Louisiana_Waterthrush_0058_177388.jpg\n",
      "  - Dimensions: 500 x 315\n",
      "  - Channels: 3\n",
      "  - File size: 129736 bytes\n",
      "\n",
      "Image: Lincoln_Sparrow_0057_117334.jpg\n",
      "  - Dimensions: 500 x 334\n",
      "  - Channels: 3\n",
      "  - File size: 147061 bytes\n",
      "\n",
      "Image: Blue_Winged_Warbler_0063_161810.jpg\n",
      "  - Dimensions: 500 x 383\n",
      "  - Channels: 3\n",
      "  - File size: 150253 bytes\n",
      "\n",
      "Image: Mockingbird_0057_79643.jpg\n",
      "  - Dimensions: 500 x 379\n",
      "  - Channels: 3\n",
      "  - File size: 81786 bytes\n",
      "\n",
      "Image: Fish_Crow_0010_25836.jpg\n",
      "  - Dimensions: 500 x 332\n",
      "  - Channels: 3\n",
      "  - File size: 65682 bytes\n",
      "\n",
      "Image: Blue_Grosbeak_0104_36984.jpg\n",
      "  - Dimensions: 500 x 315\n",
      "  - Channels: 3\n",
      "  - File size: 156598 bytes\n",
      "\n",
      "Image: Acadian_Flycatcher_0019_795592.jpg\n",
      "  - Dimensions: 500 x 346\n",
      "  - Channels: 3\n",
      "  - File size: 49648 bytes\n",
      "\n",
      "Image: Rhinoceros_Auklet_0035_2166.jpg\n",
      "  - Dimensions: 500 x 333\n",
      "  - Channels: 3\n",
      "  - File size: 98302 bytes\n",
      "\n",
      "Image: Black_Footed_Albatross_0060_796076.jpg\n",
      "  - Dimensions: 320 x 240\n",
      "  - Channels: 3\n",
      "  - File size: 24497 bytes\n",
      "\n",
      "Image: Tennessee_Warbler_0023_174977.jpg\n",
      "  - Dimensions: 500 x 333\n",
      "  - Channels: 3\n",
      "  - File size: 76849 bytes\n",
      "\n",
      "Image: Evening_Grosbeak_0016_37613.jpg\n",
      "  - Dimensions: 500 x 357\n",
      "  - Channels: 3\n",
      "  - File size: 114406 bytes\n",
      "\n",
      "Image: Ringed_Kingfisher_0005_73086.jpg\n",
      "  - Dimensions: 500 x 357\n",
      "  - Channels: 3\n",
      "  - File size: 105588 bytes\n",
      "\n",
      "Image: Herring_Gull_0103_45996.jpg\n",
      "  - Dimensions: 500 x 332\n",
      "  - Channels: 3\n",
      "  - File size: 71487 bytes\n",
      "\n",
      "Image: Red_Cockaded_Woodpecker_0029_794724.jpg\n",
      "  - Dimensions: 500 x 435\n",
      "  - Channels: 3\n",
      "  - File size: 79015 bytes\n",
      "\n",
      "Image: Tree_Sparrow_0113_123613.jpg\n",
      "  - Dimensions: 500 x 400\n",
      "  - Channels: 3\n",
      "  - File size: 108468 bytes\n",
      "\n",
      "Image: Brown_Pelican_0141_94533.jpg\n",
      "  - Dimensions: 375 x 500\n",
      "  - Channels: 3\n",
      "  - File size: 87118 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function to collect all image paths from nested subdirectories\n",
    "def get_image_paths(folder):\n",
    "    image_paths = []\n",
    "    # Traverse the directory and subdirectories\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for image in files:\n",
    "            # Check if the file is an image by its extension\n",
    "            if image.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff')):\n",
    "                image_paths.append(os.path.join(root, image))\n",
    "    return image_paths  # Return the list of image paths\n",
    "\n",
    "# function to check image dimensions and size of a certain amt of images\n",
    "def check_random_image_sizes(folder, num_samples=10):\n",
    "    # Get all image files from nested subdirectories\n",
    "    image_paths = get_image_paths(folder)\n",
    "    \n",
    "    # check if there are enough images to sample (there are lol)\n",
    "    if len(image_paths) < num_samples:\n",
    "        print(f\"Only found {len(image_paths)} images, processing all of them.\")\n",
    "        selected_images = image_paths\n",
    "    else:\n",
    "        # randomly select a subset of images\n",
    "        selected_images = random.sample(image_paths, num_samples)\n",
    "    \n",
    "    # process each selected image\n",
    "    for file_path in selected_images:\n",
    "        try:\n",
    "            # read the image using OpenCV\n",
    "            img = cv2.imread(file_path)\n",
    "            \n",
    "            # check if image was loaded correctly\n",
    "            if img is not None:\n",
    "                # get dimensions: height, width, and number of channels\n",
    "                height, width, channels = img.shape\n",
    "                \n",
    "                # get file size in bytes\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                \n",
    "                print(f\"Image: {os.path.basename(file_path)}\")\n",
    "                print(f\"  - Dimensions: {width} x {height}\")\n",
    "                print(f\"  - Channels: {channels}\")\n",
    "                print(f\"  - File size: {file_size} bytes\\n\")\n",
    "            else:\n",
    "                print(f\"Could not read image: {os.path.basename(file_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {os.path.basename(file_path)}: {e}\")\n",
    "\n",
    "# run the function to check 10 random images from Train and Test directories\n",
    "check_random_image_sizes(Train_images, num_samples=10)\n",
    "check_random_image_sizes(Test_images, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da91b7ba-6ef6-4aef-a41c-8a92d4d63ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eef65c5a-af8b-4fc1-91b6-1aeb976e9526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base this off the model u use\n",
    "IMG_SIZE = 300\n",
    "\n",
    "SIZE = (IMG_SIZE, IMG_SIZE)\n",
    "# base this off the nu,ber of classification species\n",
    "NUM_CLASSES = 200\n",
    "\n",
    "# now we make a validation set too, using image generator\n",
    "data_gen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    # we want to make a validation set too, make it abt 20% of training data\n",
    "    validation_split = .2,\n",
    "    # this parameter, to my understanding, is to help the model better view distorted images / make the model more robust\n",
    "    shear_range = .2,\n",
    "    zoom_range = .2,\n",
    "    horizontal_flip = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9134a063-9756-44cb-ac41-c02b47328553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4869 images belonging to 200 classes.\n",
      "Found 1125 images belonging to 200 classes.\n",
      "Found 5794 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "# lets apply the data generator to the train set\n",
    "train_data = data_gen.flow_from_directory(\n",
    "    # remember we have the file paths defined above so we use them here\n",
    "    Train_images,\n",
    "    target_size = SIZE,\n",
    "    color_mode='rgb',\n",
    "    # from what i understand, this will be the amount of training cycle iterations\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical',\n",
    "    subset = 'training',\n",
    "    # pictures shown in random order\n",
    "    shuffle = True,\n",
    "    # seed is 42 to keep consistency\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "val_data = data_gen.flow_from_directory(\n",
    "    # remember we have the file paths defined above so we use them here\n",
    "    Train_images,\n",
    "    target_size = SIZE,\n",
    "    color_mode='rgb',\n",
    "    # from what i understand, this will be the amount of training cycle iterations\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical',\n",
    "    # only real change between this and train data above\n",
    "    subset = 'validation',\n",
    "    # pictures shown in random order\n",
    "    shuffle = True,\n",
    "    # seed is 42 to keep consistency\n",
    "    seed = 42\n",
    ")\n",
    "# for test set, we keep it simple, only resize it\n",
    "test_data = ImageDataGenerator(\n",
    "    rescale = 1./255\n",
    ")\n",
    "\n",
    "# now let's load in the test data with the datagen specifictions\n",
    "test_data = test_data.flow_from_directory(\n",
    "    Test_images,\n",
    "    target_size = SIZE,\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical',\n",
    "    color_mode = 'rgb',\n",
    "    # keep it in order for test images\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7349227c-9b37-412a-9af1-922561942bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">10,783,535</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">196,736</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,800</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb3 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m1536\u001b[0m)   │    \u001b[38;5;34m10,783,535\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m196,736\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │        \u001b[38;5;34m25,800\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,006,071</span> (41.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,006,071\u001b[0m (41.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,918,768</span> (41.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,918,768\u001b[0m (41.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">87,303</span> (341.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m87,303\u001b[0m (341.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# now lets built the basic model\n",
    "base_model = EfficientNetB3(\n",
    "    input_shape = (IMG_SIZE, IMG_SIZE, 3),\n",
    "    # we choose False when we want to fine tune more intensly\n",
    "    include_top = False,\n",
    "    weights = 'imagenet'\n",
    ")\n",
    "\n",
    "#we're adding custom layers on top of model\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation = 'relu'),\n",
    "    # this should help with overfitting\n",
    "    layers.Dropout(.5),\n",
    "    layers.Dense(NUM_CLASSES, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# now lets compile\n",
    "model.compile(\n",
    "    optimizer = optimizers.Adam(0.0005),\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b09f512-c34f-4f33-9d68-11c14d6a0065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 23:49:01.829763: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "/opt/anaconda3/envs/CNN_work/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  2/153\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m47:08\u001b[0m 19s/step - accuracy: 0.0000e+00 - loss: 5.3603  "
     ]
    }
   ],
   "source": [
    "# now we train omg\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    # i dont think we neeed this: steps_per_epoch = len(train_images),\n",
    "    # will iterate over training data ten times\n",
    "    epochs = 10,\n",
    "    #model uses it to test performance each time\n",
    "    validation_data = val_data,\n",
    "    callbacks = [\n",
    "        # will stop model if it stops improving, prevents overfitting\n",
    "        EarlyStopping(monitor = 'val_loss',\n",
    "                      # waits 5 epochs before stopping\n",
    "                      patience = 5,\n",
    "                      # after stopping it gets best parameters\n",
    "                      restore_best_weights = True),\n",
    "        # redices learning rate when validation stops\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, mode='min')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3d543-985b-411f-a75e-943f7f12a71f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNN_work",
   "language": "python",
   "name": "cnn_work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
